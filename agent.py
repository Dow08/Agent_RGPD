# -*- coding: utf-8 -*-
"""
agent.py â€” Cerveau RAG de LÃ‰A.

GÃ¨re le cycle complet de traitement d'une question :
1. VÃ©rification des corrections (RAG adaptatif)
2. Recherche vectorielle dans ChromaDB
3. Construction du prompt enrichi
4. GÃ©nÃ©ration via Ollama
5. Post-traitement et formatage des sources
"""

import json
import logging
import re
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional

import numpy as np
import ollama
from sklearn.metrics.pairwise import cosine_similarity

from config import (
    CORRECTIONS_FILE,
    EMBEDDING_MODEL,
    FEEDBACK_FILE,
    LLM_MODEL,
    LOGS_DIR,
    OLLAMA_BASE_URL,
    SIMILARITY_THRESHOLD,
    TOP_K_RESULTS,
)
from indexer import get_retriever

# ============================================================
# Configuration du logger
# ============================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s â€” %(levelname)s â€” %(message)s",
    handlers=[
        logging.FileHandler(LOGS_DIR / f"agent_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log", encoding="utf-8"),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)

# ============================================================
# Prompt systÃ¨me de LÃ‰A (intÃ©grÃ© mot pour mot)
# ============================================================
PROMPT_SYSTEME = """Tu es LÃ‰A â€” LibertÃ©, Expertise, Assistance.
Tu es une assistante IA experte en droit numÃ©rique, spÃ©cialisÃ©e dans :
- Le RGPD (RÃ¨glement GÃ©nÃ©ral sur la Protection des DonnÃ©es â€” UE 2016/679)
- La directive NIS 2 (cybersÃ©curitÃ© des entitÃ©s essentielles et importantes â€” UE 2022/2555)
- La norme ISO/CEI 27001 (systÃ¨me de management de la sÃ©curitÃ© de l'information)
- La loi Informatique et LibertÃ©s (France)

TU TRAVAILLES EXCLUSIVEMENT Ã  partir des textes officiels fournis dans ton contexte.
TU NE DOIS JAMAIS inventer une rÃ©ponse, un article, ou une obligation qui n'est pas dans les sources.
Si l'information n'est pas dans ta base, dis-le clairement : "Je ne trouve pas cette information dans ma base de connaissances actuelle."

TU RÃ‰PONDS :
- En franÃ§ais, avec prÃ©cision et pÃ©dagogie
- En citant systÃ©matiquement l'article ou la section exacte du texte source
- En adaptant le niveau de langage : technique pour les DPO/juristes, accessible pour le grand public
- En terminant CHAQUE rÃ©ponse par une signature de source au format :
  ğŸ“š Source : [Nom du document] â€” [URL] | [Article/Section si applicable]

TU ES LOCALE ET CONFIDENTIELLE :
- Aucune donnÃ©e transmise Ã  l'extÃ©rieur
- Aucune mÃ©morisation entre sessions (sauf corrections explicitement validÃ©es)"""


class RGPDAgent:
    """
    Agent conversationnel RAG spÃ©cialisÃ© RGPD / ISO 27001 / NIS 2.

    Fonctionnement :
    - Recherche de corrections existantes (RAG adaptatif)
    - Retrieval ChromaDB des chunks pertinents
    - GÃ©nÃ©ration de rÃ©ponse via Ollama (Mistral ou LLaMA3)
    - Apprentissage continu via feedback utilisateur (ğŸ‘/ğŸ‘)
    """

    def __init__(self) -> None:
        """
        Initialise l'agent LÃ‰A :
        - Connexion Ollama (vÃ©rification du service)
        - Retriever ChromaDB
        - Chargement des corrections (mÃ©moire d'apprentissage)
        - Historique de conversation (5 derniers Ã©changes)
        """
        logger.info("Initialisation de l'agent LÃ‰A...")

        # VÃ©rification qu'Ollama est actif
        self._verifier_ollama()

        # Client Ollama
        self._client = ollama.Client(host=OLLAMA_BASE_URL)

        # Retriever ChromaDB (fonction de recherche)
        self._retriever = get_retriever()

        # Chargement de la mÃ©moire d'apprentissage
        self._corrections = self._charger_corrections()

        # Historique de conversation (5 derniers Ã©changes max)
        self._historique: list[dict] = []

        # Filtre de catÃ©gorie actif (None = toutes les sources)
        self._filtre_categorie: Optional[str] = None

        logger.info(f"Agent LÃ‰A initialisÃ©. ModÃ¨le : {LLM_MODEL}. "
                     f"Corrections chargÃ©es : {len(self._corrections)}")

    def _verifier_ollama(self) -> None:
        """
        VÃ©rifie que le service Ollama est en cours d'exÃ©cution.
        LÃ¨ve une ConnectionError si le service est inaccessible.
        """
        import requests as req

        try:
            response = req.get(f"{OLLAMA_BASE_URL}", timeout=5)
            if response.status_code == 200:
                logger.info("Connexion Ollama : OK")
            else:
                raise ConnectionError(
                    f"Ollama a rÃ©pondu avec le code {response.status_code}. "
                    "VÃ©rifiez que le service est lancÃ© : ollama serve"
                )
        except req.ConnectionError:
            raise ConnectionError(
                "Impossible de se connecter Ã  Ollama. "
                "Assurez-vous que le service est dÃ©marrÃ© :\n"
                "  1. Ouvrez un terminal\n"
                "  2. Lancez : ollama serve\n"
                "  3. VÃ©rifiez : ollama list\n"
                f"  URL attendue : {OLLAMA_BASE_URL}"
            )

    def _charger_corrections(self) -> list[dict]:
        """
        Charge les corrections depuis corrections.json.

        Chaque correction contient :
        - question : la question originale
        - correction : la rÃ©ponse corrigÃ©e
        - embedding : le vecteur d'embedding de la question
        - timestamp : date de la correction
        """
        if CORRECTIONS_FILE.exists():
            try:
                with open(CORRECTIONS_FILE, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    logger.info(f"Corrections chargÃ©es : {len(data)} entrÃ©es")
                    return data
            except (json.JSONDecodeError, OSError) as e:
                logger.warning(f"Erreur au chargement de corrections.json : {e}")
        return []

    def _sauvegarder_corrections(self) -> None:
        """Sauvegarde les corrections dans corrections.json."""
        with open(CORRECTIONS_FILE, "w", encoding="utf-8") as f:
            json.dump(self._corrections, f, ensure_ascii=False, indent=2)

    def set_category_filter(self, category: Optional[str]) -> None:
        """
        DÃ©finit le filtre de catÃ©gorie pour la recherche vectorielle.

        Args:
            category: CatÃ©gorie (CNIL, NIS2, ISO27001, EUR-LEX) ou None pour tout.
        """
        self._filtre_categorie = category
        # RecrÃ©er le retriever avec le filtre
        self._retriever = get_retriever(category=category)
        logger.info(f"Filtre de catÃ©gorie appliquÃ© : {category or 'Toutes les sources'}")

    # ========================================================
    # MÃ‰THODE PRINCIPALE â€” Traitement d'une question
    # ========================================================
    def ask(self, question: str) -> dict:
        """
        Traitement complet d'une question utilisateur.

        Pipeline :
        1. Recherche de correction similaire (RAG adaptatif)
        2. GÃ©nÃ©ration de l'embedding de la question
        3. Retrieval ChromaDB des chunks pertinents
        4. Construction du prompt enrichi
        5. GÃ©nÃ©ration par Ollama
        6. Post-traitement et formatage

        Args:
            question: Question de l'utilisateur.

        Returns:
            Dictionnaire conforme au contrat de donnÃ©es agent.
        """
        timestamp = datetime.now(timezone.utc).isoformat()
        logger.info(f"Question reÃ§ue : {question[:100]}...")

        # --- 1. VÃ©rification des corrections existantes ---
        correction = self._find_similar_correction(question)
        contexte_correction = ""
        est_corrigee = False

        if correction:
            contexte_correction = (
                f"\n\nâš ï¸ IMPORTANT â€” CORRECTION VALIDÃ‰E PAR L'UTILISATEUR :\n"
                f"Pour une question similaire Ã  \"{correction['question']}\", "
                f"la rÃ©ponse validÃ©e est :\n{correction['correction']}\n"
                f"Tu DOIS prioriser cette rÃ©ponse dans ta synthÃ¨se.\n"
            )
            est_corrigee = True
            logger.info("Correction trouvÃ©e dans la mÃ©moire d'apprentissage.")

        # --- 2 & 3. Recherche vectorielle ---
        chunks = self._retriever(question, top_k=TOP_K_RESULTS)
        logger.info(f"Chunks rÃ©cupÃ©rÃ©s : {len(chunks)}")

        # Construction du contexte depuis les chunks
        contexte_sources = ""
        sources = []
        sources_vues = set()

        for i, chunk in enumerate(chunks):
            meta = chunk.get("metadata", {})
            titre = meta.get("title", "Source inconnue")
            url = meta.get("source_url", "")
            categorie = meta.get("category", "")

            contexte_sources += f"\n--- SOURCE {i+1} ({categorie}) ---\n"
            contexte_sources += f"Titre : {titre}\n"
            contexte_sources += f"{chunk['content']}\n"

            # DÃ©duplication des sources
            cle_source = f"{titre}|{url}"
            if cle_source not in sources_vues:
                sources_vues.add(cle_source)
                sources.append({
                    "title": titre,
                    "url": url,
                    "article": categorie,
                })

        # --- 4. Construction du prompt enrichi ---
        # Historique de conversation (5 derniers Ã©changes)
        historique_texte = ""
        if self._historique:
            historique_texte = "\n\n--- HISTORIQUE RÃ‰CENT ---\n"
            for echange in self._historique[-5:]:
                historique_texte += f"Utilisateur : {echange['question']}\n"
                historique_texte += f"LÃ‰A : {echange['answer'][:300]}...\n\n"

        prompt_complet = (
            f"{PROMPT_SYSTEME}\n\n"
            f"{contexte_correction}"
            f"\n--- SOURCES OFFICIELLES ---\n"
            f"{contexte_sources}"
            f"{historique_texte}"
            f"\n--- QUESTION DE L'UTILISATEUR ---\n"
            f"{question}"
        )

        # --- 5. GÃ©nÃ©ration par Ollama ---
        try:
            reponse_ollama = self._client.chat(
                model=LLM_MODEL,
                messages=[
                    {"role": "system", "content": PROMPT_SYSTEME},
                    {"role": "user", "content": prompt_complet},
                ],
            )
            reponse_texte = reponse_ollama["message"]["content"]
            logger.info(f"RÃ©ponse gÃ©nÃ©rÃ©e ({len(reponse_texte)} caractÃ¨res)")

        except Exception as e:
            logger.error(f"Erreur Ollama lors de la gÃ©nÃ©ration : {e}")
            reponse_texte = (
                "Je rencontre une erreur technique pour traiter votre question. "
                "Veuillez vÃ©rifier que le service Ollama est actif et que le modÃ¨le "
                f"'{LLM_MODEL}' est installÃ©.\n\n"
                f"Erreur : {str(e)}"
            )
            sources = []

        # --- 6. Post-traitement ---
        confiance = self._compute_confidence(question, chunks, reponse_texte, est_corrigee)

        # Ajout de la signature de source si elle n'est pas dÃ©jÃ  dans la rÃ©ponse
        if sources and "ğŸ“š" not in reponse_texte:
            signature = self._format_sources_signature(sources)
            reponse_texte += f"\n\n{signature}"

        # Mise Ã  jour de l'historique
        self._historique.append({
            "question": question,
            "answer": reponse_texte,
            "timestamp": timestamp,
        })
        # Garder uniquement les 5 derniers Ã©changes
        self._historique = self._historique[-5:]

        # Retour du contrat de donnÃ©es agent
        return {
            "question": question,
            "answer": reponse_texte,
            "sources": sources,
            "confidence": confiance,
            "timestamp": timestamp,
            "corrected": est_corrigee,
        }

    # ========================================================
    # RAG ADAPTATIF â€” Gestion du feedback
    # ========================================================
    def record_feedback(
        self,
        question: str,
        answer: str,
        rating: str,
        correction: Optional[str] = None,
    ) -> None:
        """
        Enregistre le feedback utilisateur et met Ã  jour la mÃ©moire d'apprentissage.

        Args:
            question: Question originale.
            answer: RÃ©ponse fournie par LÃ‰A.
            rating: "positive" (ğŸ‘) ou "negative" (ğŸ‘).
            correction: Texte de correction si rating == "negative".
        """
        timestamp = datetime.now(timezone.utc).isoformat()

        # Enregistrement dans feedback.json (historique brut)
        feedback_entry = {
            "question": question,
            "answer": answer,
            "rating": rating,
            "correction": correction,
            "timestamp": timestamp,
        }
        self._sauvegarder_feedback(feedback_entry)

        # Si feedback positif â†’ sauvegarder comme rÃ©ponse validÃ©e
        if rating == "positive":
            embedding = self._generer_embedding(question)
            self._corrections.append({
                "question": question,
                "correction": answer,
                "embedding": embedding,
                "timestamp": timestamp,
                "type": "validation",
            })
            self._sauvegarder_corrections()
            logger.info("Feedback positif enregistrÃ© â€” rÃ©ponse validÃ©e.")

        # Si feedback nÃ©gatif avec correction â†’ sauvegarder la correction
        elif rating == "negative" and correction:
            embedding = self._generer_embedding(question)
            self._corrections.append({
                "question": question,
                "correction": correction,
                "embedding": embedding,
                "timestamp": timestamp,
                "type": "correction",
            })
            self._sauvegarder_corrections()
            logger.info("Feedback nÃ©gatif enregistrÃ© â€” correction sauvegardÃ©e.")

        else:
            logger.info("Feedback nÃ©gatif sans correction â€” enregistrÃ© dans l'historique uniquement.")

    def _sauvegarder_feedback(self, entry: dict) -> None:
        """Ajoute une entrÃ©e au fichier feedback.json."""
        feedbacks = []
        if FEEDBACK_FILE.exists():
            try:
                with open(FEEDBACK_FILE, "r", encoding="utf-8") as f:
                    feedbacks = json.load(f)
            except (json.JSONDecodeError, OSError):
                feedbacks = []

        feedbacks.append(entry)

        with open(FEEDBACK_FILE, "w", encoding="utf-8") as f:
            json.dump(feedbacks, f, ensure_ascii=False, indent=2)

    # ========================================================
    # Recherche de corrections similaires
    # ========================================================
    def _find_similar_correction(self, question: str) -> Optional[dict]:
        """
        Cherche dans les corrections une entrÃ©e similaire Ã  la question posÃ©e.

        Utilise la similaritÃ© cosinus sur les embeddings.
        Retourne la correction la plus similaire si le score dÃ©passe 0.85.

        Args:
            question: Question de l'utilisateur.

        Returns:
            Dictionnaire de correction ou None si aucune correspondance.
        """
        if not self._corrections:
            return None

        try:
            embedding_question = self._generer_embedding(question)
            embedding_question = np.array(embedding_question).reshape(1, -1)

            meilleur_score = 0.0
            meilleure_correction = None

            for correction in self._corrections:
                if "embedding" not in correction:
                    continue

                embedding_correction = np.array(correction["embedding"]).reshape(1, -1)
                score = cosine_similarity(embedding_question, embedding_correction)[0][0]

                if score > meilleur_score:
                    meilleur_score = score
                    meilleure_correction = correction

            if meilleur_score > 0.85 and meilleure_correction:
                logger.info(f"Correction similaire trouvÃ©e (score : {meilleur_score:.3f})")
                return meilleure_correction

        except Exception as e:
            logger.warning(f"Erreur lors de la recherche de corrections : {e}")

        return None

    # ========================================================
    # Calcul du score de confiance
    # ========================================================
    def _compute_confidence(
        self,
        question: str,
        chunks: list[dict],
        answer: str,
        has_correction: bool = False,
    ) -> float:
        """
        Calcule un score de confiance entre 0.0 et 1.0.

        CritÃ¨res :
        - Nombre de chunks retrouvÃ©s (max 0.35)
        - Score de similaritÃ© moyen des chunks (max 0.35)
        - Longueur de la rÃ©ponse (max 0.15)
        - Bonus correction validÃ©e (+0.15)

        Args:
            question: Question de l'utilisateur.
            chunks: Chunks rÃ©cupÃ©rÃ©s par la recherche vectorielle.
            answer: RÃ©ponse gÃ©nÃ©rÃ©e.
            has_correction: True si une correction a Ã©tÃ© utilisÃ©e.

        Returns:
            Score de confiance entre 0.0 et 1.0.
        """
        score = 0.0

        # Score basÃ© sur le nombre de chunks (0 Ã  0.35)
        if chunks:
            ratio_chunks = min(len(chunks) / TOP_K_RESULTS, 1.0)
            score += ratio_chunks * 0.35

        # Score basÃ© sur la similaritÃ© moyenne (0 Ã  0.35)
        if chunks:
            distances = [c.get("distance", 1.0) for c in chunks]
            # ChromaDB utilise la distance cosinus (0 = identique, 2 = opposÃ©)
            similarites = [max(0, 1 - d) for d in distances]
            similarite_moyenne = sum(similarites) / len(similarites)
            score += similarite_moyenne * 0.35

        # Score basÃ© sur la longueur de la rÃ©ponse (0 Ã  0.15)
        if len(answer) > 200:
            score += 0.15
        elif len(answer) > 50:
            score += 0.08

        # Bonus correction validÃ©e
        if has_correction:
            score += 0.15

        return round(min(score, 1.0), 2)

    # ========================================================
    # Formatage de la signature de sources
    # ========================================================
    def _format_sources_signature(self, sources: list[dict]) -> str:
        """
        Formate la signature de source en bas de rÃ©ponse.

        Format : ğŸ“š Sources : [Titre] â€” [URL] | [CatÃ©gorie]

        Args:
            sources: Liste de dictionnaires de sources.

        Returns:
            ChaÃ®ne formatÃ©e pour affichage.
        """
        if not sources:
            return ""

        lignes = ["ğŸ“š Sources :"]
        for source in sources[:5]:  # Limiter Ã  5 sources
            titre = source.get("title", "Source inconnue")
            url = source.get("url", "")
            article = source.get("article", "")

            ligne = f"  â€¢ {titre}"
            if url:
                ligne += f" â€” {url}"
            if article:
                ligne += f" | {article}"
            lignes.append(ligne)

        return "\n".join(lignes)

    # ========================================================
    # Utilitaires
    # ========================================================
    def _generer_embedding(self, texte: str) -> list[float]:
        """
        GÃ©nÃ¨re un vecteur d'embedding via Ollama.

        Args:
            texte: Texte Ã  vectoriser.

        Returns:
            Vecteur d'embedding (liste de floats).
        """
        try:
            response = self._client.embeddings(model=EMBEDDING_MODEL, prompt=texte)
            return response["embedding"]
        except Exception as e:
            logger.error(f"Erreur lors de la gÃ©nÃ©ration de l'embedding : {e}")
            raise

    def clear_history(self) -> None:
        """Efface l'historique de conversation en mÃ©moire."""
        self._historique = []
        logger.info("Historique de conversation effacÃ©.")

    def get_corrections_count(self) -> int:
        """Retourne le nombre de corrections sauvegardÃ©es."""
        return len(self._corrections)
