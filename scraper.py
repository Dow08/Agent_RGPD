# -*- coding: utf-8 -*-
"""
scraper.py ‚Äî Moteur de scraping multi-sources pour L√âA.

Collecte les contenus des sites officiels (CNIL, ANSSI, EUR-Lex, etc.)
en respectant les robots.txt, le rate limiting et la whitelist de domaines.
Utilise BeautifulSoup en priorit√© avec fallback Selenium si n√©cessaire.
"""

import hashlib
import json
import logging
import random
import re
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional
from urllib.parse import urljoin, urlparse
from urllib.robotparser import RobotFileParser

import requests
from bs4 import BeautifulSoup
from tqdm import tqdm

from config import (
    ALLOWED_DOMAINS,
    HISTORY_FILE,
    LOGS_DIR,
    PATTERNS_FILE,
    RAW_DIR,
    SCRAPE_DELAY,
    SOURCE_URLS,
    USER_AGENTS,
)

# ============================================================
# Configuration du logger
# ============================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s ‚Äî %(levelname)s ‚Äî %(message)s",
    handlers=[
        logging.FileHandler(LOGS_DIR / f"scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log", encoding="utf-8"),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)


# ============================================================
# S√©lecteurs CSS par domaine (extraction intelligente)
# ============================================================
DOMAIN_SELECTORS: dict[str, list[str]] = {
    "cnil.fr": ["article.content", "div.field-item", "article", "main"],
    "cyber.gouv.fr": ["main", "article", "div.content"],
    "monespacenis2.cyber.gouv.fr": ["main", "article", "div.content"],
    "eur-lex.europa.eu": ["div#text", "div.eli-main-title", "div#TexteOnly", "article"],
    "fr.wikipedia.org": ["div#mw-content-text", "div.mw-parser-output"],
    "advisera.com": ["article", "div.entry-content", "main"],
}

# S√©lecteurs de repli universels
FALLBACK_SELECTORS: list[str] = ["main", "article", "body"]


# ============================================================
# Gestion de la m√©moire des patterns (parsing_patterns.json)
# ============================================================
def charger_patterns() -> dict:
    """Charge les patterns CSS m√©moris√©s depuis le fichier JSON."""
    if PATTERNS_FILE.exists():
        try:
            with open(PATTERNS_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, OSError):
            logger.warning("Fichier parsing_patterns.json corrompu, r√©initialisation.")
    return {}


def sauvegarder_pattern(domaine: str, selecteur: str) -> None:
    """Sauvegarde un s√©lecteur CSS valid√© pour un domaine donn√©."""
    patterns = charger_patterns()
    if domaine not in patterns:
        patterns[domaine] = []

    # V√©rifier si le s√©lecteur existe d√©j√†
    for p in patterns[domaine]:
        if p["selecteur"] == selecteur:
            p["derniere_utilisation"] = datetime.now(timezone.utc).isoformat()
            p["deprecated"] = False
            break
    else:
        patterns[domaine].append({
            "selecteur": selecteur,
            "derniere_utilisation": datetime.now(timezone.utc).isoformat(),
            "deprecated": False,
        })

    with open(PATTERNS_FILE, "w", encoding="utf-8") as f:
        json.dump(patterns, f, ensure_ascii=False, indent=2)


def marquer_pattern_deprecie(domaine: str, selecteur: str) -> None:
    """Marque un pattern comme d√©pr√©ci√© apr√®s un √©chec."""
    patterns = charger_patterns()
    if domaine in patterns:
        for p in patterns[domaine]:
            if p["selecteur"] == selecteur:
                p["deprecated"] = True
                break
        with open(PATTERNS_FILE, "w", encoding="utf-8") as f:
            json.dump(patterns, f, ensure_ascii=False, indent=2)


def obtenir_selecteurs_pour_domaine(domaine: str) -> list[str]:
    """
    Retourne les s√©lecteurs CSS pour un domaine donn√©.
    Priorit√© : patterns m√©moris√©s non d√©pr√©ci√©s > domaine connu > fallback.
    """
    selecteurs = []

    # 1. Patterns m√©moris√©s valid√©s en priorit√©
    patterns = charger_patterns()
    if domaine in patterns:
        for p in patterns[domaine]:
            if not p.get("deprecated", False):
                selecteurs.append(p["selecteur"])

    # 2. S√©lecteurs connus pour le domaine
    for cle_domaine, sels in DOMAIN_SELECTORS.items():
        if cle_domaine in domaine:
            for s in sels:
                if s not in selecteurs:
                    selecteurs.append(s)
            break

    # 3. Fallback universel
    for s in FALLBACK_SELECTORS:
        if s not in selecteurs:
            selecteurs.append(s)

    return selecteurs


# ============================================================
# Validation d'URL et v√©rification de s√©curit√©
# ============================================================
def valider_url(url: str) -> bool:
    """
    Valide qu'une URL est s√ªre √† scraper :
    - Sch√©ma HTTPS uniquement
    - Domaine dans la whitelist ALLOWED_DOMAINS
    """
    try:
        parsed = urlparse(url)
    except Exception:
        return False

    # Sch√©ma HTTPS obligatoire
    if parsed.scheme != "https":
        logger.debug(f"URL rejet√©e (sch√©ma non HTTPS) : {url}")
        return False

    # V√©rification du domaine dans la whitelist
    domaine = parsed.netloc.lower().replace("www.", "")
    for domaine_autorise in ALLOWED_DOMAINS:
        if domaine == domaine_autorise or domaine.endswith(f".{domaine_autorise}"):
            return True

    logger.debug(f"URL rejet√©e (domaine non autoris√©) : {url}")
    return False


def verifier_robots_txt(url: str) -> bool:
    """
    V√©rifie que le fichier robots.txt autorise l'acc√®s √† l'URL.
    En cas d'erreur de lecture du robots.txt, on autorise par d√©faut.
    """
    try:
        parsed = urlparse(url)
        robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"

        rp = RobotFileParser()
        rp.set_url(robots_url)
        rp.read()

        autorise = rp.can_fetch("*", url)
        if not autorise:
            logger.info(f"robots.txt interdit l'acc√®s √† : {url}")
        return autorise
    except Exception as e:
        logger.warning(f"Impossible de lire robots.txt pour {url} : {e}. Acc√®s autoris√© par d√©faut.")
        return True


def obtenir_domaine(url: str) -> str:
    """Extrait le domaine d'une URL (sans www.)."""
    return urlparse(url).netloc.lower().replace("www.", "")


def obtenir_user_agent() -> str:
    """Retourne un User-Agent al√©atoire depuis la liste configur√©e."""
    return random.choice(USER_AGENTS)


# ============================================================
# Scraper dynamique Selenium (fallback)
# ============================================================
class DynamicScraper:
    """
    Scraper Selenium headless utilis√© comme fallback
    lorsque BeautifulSoup ne r√©cup√®re pas assez de contenu (< 200 caract√®res).
    """

    def __init__(self) -> None:
        """Initialise le driver Chrome en mode headless."""
        self._driver = None

    def _initialiser_driver(self) -> None:
        """D√©marre le navigateur Chrome headless avec webdriver-manager."""
        if self._driver is not None:
            return

        try:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            from selenium.webdriver.chrome.service import Service
            from webdriver_manager.chrome import ChromeDriverManager

            options = Options()
            options.add_argument("--headless=new")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--disable-gpu")
            options.add_argument(f"user-agent={obtenir_user_agent()}")
            options.page_load_strategy = "normal"

            service = Service(ChromeDriverManager().install())
            self._driver = webdriver.Chrome(service=service, options=options)
            self._driver.set_page_load_timeout(15)
            logger.info("Driver Selenium initialis√© avec succ√®s.")
        except Exception as e:
            logger.error(f"Impossible d'initialiser Selenium : {e}")
            self._driver = None
            raise

    def scrape(self, url: str) -> str:
        """
        R√©cup√®re le HTML brut d'une page via Selenium.

        Args:
            url: URL de la page √† scraper.

        Returns:
            Le code HTML source de la page.
        """
        self._initialiser_driver()
        if self._driver is None:
            raise RuntimeError("Driver Selenium non disponible.")

        logger.info(f"[Selenium] Chargement de : {url}")
        self._driver.get(url)
        time.sleep(2)  # Attente du rendu JavaScript
        return self._driver.page_source

    def close(self) -> None:
        """Lib√®re le driver Selenium proprement."""
        if self._driver is not None:
            try:
                self._driver.quit()
                logger.info("Driver Selenium ferm√© proprement.")
            except Exception as e:
                logger.warning(f"Erreur lors de la fermeture du driver : {e}")
            finally:
                self._driver = None


# ============================================================
# Instance globale du scraper dynamique (lazy loading)
# ============================================================
_dynamic_scraper: Optional[DynamicScraper] = None


def _obtenir_dynamic_scraper() -> DynamicScraper:
    """Retourne l'instance singleton du DynamicScraper."""
    global _dynamic_scraper
    if _dynamic_scraper is None:
        _dynamic_scraper = DynamicScraper()
    return _dynamic_scraper


# ============================================================
# Fonctions principales de scraping
# ============================================================
def scrape_page(url: str) -> dict:
    """
    Scrape une page web et retourne les donn√©es selon le contrat de donn√©es.

    Processus :
    1. Validation de l'URL (HTTPS + domaine autoris√©)
    2. V√©rification robots.txt
    3. Tentative BeautifulSoup
    4. Fallback Selenium si contenu < 200 caract√®res
    5. Extraction intelligente par s√©lecteurs CSS selon le domaine

    Args:
        url: URL de la page √† scraper.

    Returns:
        Dictionnaire conforme au contrat de donn√©es scraper.
    """
    # Validation de s√©curit√©
    if not valider_url(url):
        return {
            "status": "error",
            "url_source": url,
            "title": "",
            "content": "",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "items_count": 0,
            "domain": "",
        }

    # V√©rification robots.txt
    if not verifier_robots_txt(url):
        return {
            "status": "error",
            "url_source": url,
            "title": "",
            "content": "Acc√®s interdit par robots.txt",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "items_count": 0,
            "domain": obtenir_domaine(url),
        }

    domaine = obtenir_domaine(url)
    selecteurs = obtenir_selecteurs_pour_domaine(domaine)
    timestamp = datetime.now(timezone.utc).isoformat()

    # --- Tentative avec BeautifulSoup ---
    contenu = ""
    titre = ""
    selecteur_utilise = ""

    try:
        headers = {"User-Agent": obtenir_user_agent()}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        response.encoding = response.apparent_encoding or "utf-8"

        soup = BeautifulSoup(response.text, "lxml")

        # Extraction du titre
        tag_titre = soup.find("title")
        titre = tag_titre.get_text(strip=True) if tag_titre else ""

        # Extraction du contenu avec les s√©lecteurs
        for sel in selecteurs:
            element = soup.select_one(sel)
            if element:
                texte = _nettoyer_texte(element.get_text(separator="\n", strip=True))
                if len(texte) > len(contenu):
                    contenu = texte
                    selecteur_utilise = sel

    except requests.RequestException as e:
        logger.warning(f"Erreur BeautifulSoup pour {url} : {e}")

    # --- Fallback Selenium si contenu insuffisant ---
    if len(contenu) < 200:
        logger.info(f"Contenu insuffisant ({len(contenu)} car.), fallback Selenium pour : {url}")
        try:
            ds = _obtenir_dynamic_scraper()
            html = ds.scrape(url)
            soup = BeautifulSoup(html, "lxml")

            if not titre:
                tag_titre = soup.find("title")
                titre = tag_titre.get_text(strip=True) if tag_titre else ""

            for sel in selecteurs:
                element = soup.select_one(sel)
                if element:
                    texte = _nettoyer_texte(element.get_text(separator="\n", strip=True))
                    if len(texte) > len(contenu):
                        contenu = texte
                        selecteur_utilise = sel

        except Exception as e:
            logger.error(f"√âchec Selenium pour {url} : {e}")

    # M√©morisation du pattern utilis√©
    if selecteur_utilise and len(contenu) >= 200:
        sauvegarder_pattern(domaine, selecteur_utilise)
    elif selecteur_utilise:
        marquer_pattern_deprecie(domaine, selecteur_utilise)

    statut = "success" if len(contenu) >= 100 else "error"

    if statut == "error":
        logger.warning(f"Contenu final insuffisant pour {url} ({len(contenu)} caract√®res).")

    return {
        "status": statut,
        "url_source": url,
        "title": titre,
        "content": contenu,
        "timestamp": timestamp,
        "items_count": len(contenu.split("\n")) if contenu else 0,
        "domain": domaine,
    }


def _nettoyer_texte(texte: str) -> str:
    """
    Nettoie le texte extrait :
    - Supprime les balises HTML r√©siduelles
    - Normalise les espaces et sauts de ligne
    - Supprime les caract√®res de contr√¥le
    """
    # Suppression des balises HTML r√©siduelles
    texte = re.sub(r"<[^>]+>", "", texte)
    # Normalisation des espaces multiples
    texte = re.sub(r"[ \t]+", " ", texte)
    # Normalisation des sauts de ligne multiples
    texte = re.sub(r"\n{3,}", "\n\n", texte)
    # Suppression des espaces en d√©but/fin de ligne
    lignes = [ligne.strip() for ligne in texte.split("\n")]
    texte = "\n".join(lignes)
    return texte.strip()


def _generer_slug(titre: str) -> str:
    """
    G√©n√®re un slug √† partir du titre pour le nommage des fichiers.
    Exemple : 'Le droit d'acc√®s - CNIL' ‚Üí 'le-droit-dacces-cnil'
    """
    slug = titre.lower()
    slug = re.sub(r"[√†√°√¢√£√§√•]", "a", slug)
    slug = re.sub(r"[√®√©√™√´]", "e", slug)
    slug = re.sub(r"[√¨√≠√Æ√Ø]", "i", slug)
    slug = re.sub(r"[√≤√≥√¥√µ√∂]", "o", slug)
    slug = re.sub(r"[√π√∫√ª√º]", "u", slug)
    slug = re.sub(r"[√ß]", "c", slug)
    slug = re.sub(r"[^a-z0-9]+", "-", slug)
    slug = slug.strip("-")
    return slug[:80]  # Limiter la longueur


def _categorie_vers_prefixe(categorie: str) -> str:
    """Convertit le nom de cat√©gorie en pr√©fixe de fichier."""
    correspondances = {
        "CNIL ‚Äî RGPD": "cnil",
        "NIS 2 ‚Äî ANSSI": "nis2",
        "ISO 27001 ‚Äî Sources libres": "iso27001",
        "Textes juridiques UE ‚Äî EUR-Lex": "eurlex",
    }
    return correspondances.get(categorie, "divers")


# ============================================================
# Export en Markdown
# ============================================================
def export_to_markdown(data: dict, filepath: Path) -> None:
    """
    Exporte les donn√©es scrap√©es en fichier Markdown avec en-t√™te YAML.

    Args:
        data: Dictionnaire conforme au contrat de donn√©es scraper.
        filepath: Chemin de destination du fichier .md.
    """
    # D√©termination de la cat√©gorie depuis le domaine
    categorie = "Divers"
    domaine = data.get("domain", "")
    if "cnil" in domaine:
        categorie = "CNIL"
    elif "cyber.gouv" in domaine or "nis2" in domaine:
        categorie = "NIS2"
    elif "eur-lex" in domaine:
        categorie = "EUR-LEX"
    elif "wikipedia" in domaine or "advisera" in domaine:
        categorie = "ISO27001"

    en_tete = f"""---
title: "{data.get('title', 'Sans titre')}"
source: "{data.get('url_source', '')}"
domain: "{domaine}"
category: "{categorie}"
scraped_at: "{data.get('timestamp', '')}"
---

"""
    contenu = en_tete + data.get("content", "")

    filepath.parent.mkdir(parents=True, exist_ok=True)
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(contenu)

    logger.info(f"Export√© : {filepath.name} ({len(data.get('content', ''))} caract√®res)")


# ============================================================
# Exploration r√©cursive des liens internes
# ============================================================
def _extraire_liens_internes(url: str, html: str, domaine: str) -> list[str]:
    """
    Extrait les liens internes d'une page (m√™me domaine uniquement).

    Args:
        url: URL de la page source.
        html: Contenu HTML de la page.
        domaine: Domaine autoris√© pour le filtrage.

    Returns:
        Liste d'URLs internes uniques.
    """
    liens = []
    try:
        soup = BeautifulSoup(html, "lxml")
        for a_tag in soup.find_all("a", href=True):
            href = a_tag["href"]
            lien_complet = urljoin(url, href)

            # Filtrer : m√™me domaine, HTTPS, pas un ancre, pas un fichier binaire
            parsed = urlparse(lien_complet)
            lien_domaine = parsed.netloc.lower().replace("www.", "")

            if (
                domaine in lien_domaine
                and parsed.scheme == "https"
                and not parsed.fragment
                and not any(lien_complet.lower().endswith(ext) for ext in [".pdf", ".jpg", ".png", ".gif", ".zip", ".doc"])
            ):
                # Nettoyer l'URL (supprimer les param√®tres de tracking)
                lien_propre = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                if parsed.query:
                    # Garder uniquement les param√®tres essentiels (ex: uri= pour EUR-Lex)
                    params_essentiels = [p for p in parsed.query.split("&") if p.startswith("uri=")]
                    if params_essentiels:
                        lien_propre += "?" + "&".join(params_essentiels)

                liens.append(lien_propre)

    except Exception as e:
        logger.warning(f"Erreur extraction liens de {url} : {e}")

    return list(set(liens))


# ============================================================
# Scraping complet de toutes les sources
# ============================================================
def scrape_all_sources() -> list[dict]:
    """
    Parcourt et scrape toutes les sources officielles d√©finies dans SOURCE_URLS.

    Processus :
    - Parcours par cat√©gorie avec barre de progression
    - Exploration r√©cursive des liens internes (profondeur max 2)
    - D√©duplication par URL
    - Export en Markdown dans /data/raw/
    - Mise √† jour de l'historique de scraping

    Returns:
        Liste de tous les r√©sultats de scraping (contrats de donn√©es).
    """
    resultats: list[dict] = []
    urls_visitees: set[str] = set()
    compteurs = {"total": 0, "succes": 0, "erreurs": 0}
    stats_par_source: dict[str, dict] = {}

    logger.info("=" * 60)
    logger.info("D√âBUT DU SCRAPING DE TOUTES LES SOURCES")
    logger.info("=" * 60)

    for categorie, urls in SOURCE_URLS.items():
        prefixe = _categorie_vers_prefixe(categorie)
        date_str = datetime.now().strftime("%Y-%m-%d")
        nb_categorie = 0
        nb_erreurs_cat = 0

        logger.info(f"\nüìÇ Cat√©gorie : {categorie} ({len(urls)} URLs)")

        barre = tqdm(urls, desc=f"  {categorie}", unit="page", ncols=80)

        for url in barre:
            # D√©duplication
            if url in urls_visitees:
                continue

            # Scrape de la page principale
            resultat = scrape_page(url)
            urls_visitees.add(url)
            compteurs["total"] += 1

            if resultat["status"] == "success":
                compteurs["succes"] += 1
                nb_categorie += 1

                # Export en Markdown
                slug = _generer_slug(resultat.get("title", "sans-titre"))
                nom_fichier = f"{prefixe}_{date_str}_{slug}.md"
                chemin_fichier = RAW_DIR / nom_fichier
                export_to_markdown(resultat, chemin_fichier)

                resultats.append(resultat)

                # Exploration r√©cursive (profondeur 1 ‚Äî liens de la page)
                try:
                    headers = {"User-Agent": obtenir_user_agent()}
                    resp = requests.get(url, headers=headers, timeout=15)
                    resp.encoding = resp.apparent_encoding or "utf-8"
                    liens_internes = _extraire_liens_internes(url, resp.text, obtenir_domaine(url))

                    # Limiter √† 5 liens par page pour rester raisonnable
                    for lien in liens_internes[:5]:
                        if lien not in urls_visitees and valider_url(lien):
                            time.sleep(SCRAPE_DELAY)
                            sous_resultat = scrape_page(lien)
                            urls_visitees.add(lien)
                            compteurs["total"] += 1

                            if sous_resultat["status"] == "success":
                                compteurs["succes"] += 1
                                nb_categorie += 1
                                sous_slug = _generer_slug(sous_resultat.get("title", "sans-titre"))
                                sous_nom = f"{prefixe}_{date_str}_{sous_slug}.md"
                                sous_chemin = RAW_DIR / sous_nom

                                # √âviter les doublons de fichier
                                if not sous_chemin.exists():
                                    export_to_markdown(sous_resultat, sous_chemin)
                                    resultats.append(sous_resultat)
                            else:
                                compteurs["erreurs"] += 1
                                nb_erreurs_cat += 1

                except Exception as e:
                    logger.warning(f"Erreur exploration r√©cursive depuis {url} : {e}")

            else:
                compteurs["erreurs"] += 1
                nb_erreurs_cat += 1

            # Respect du d√©lai entre les requ√™tes
            time.sleep(SCRAPE_DELAY)

        stats_par_source[categorie] = {
            "pages_scrapees": nb_categorie,
            "erreurs": nb_erreurs_cat,
        }

        # Mise √† jour de l'historique apr√®s chaque cat√©gorie
        _mettre_a_jour_historique(categorie, nb_categorie, nb_erreurs_cat)

    # --- R√©sum√© final ---
    logger.info("\n" + "=" * 60)
    logger.info("R√âSUM√â DU SCRAPING")
    logger.info("=" * 60)
    for source, stats in stats_par_source.items():
        logger.info(f"  {source} : {stats['pages_scrapees']} pages, {stats['erreurs']} erreurs")
    logger.info(f"  TOTAL : {compteurs['total']} URLs trait√©es, "
                f"{compteurs['succes']} succ√®s, {compteurs['erreurs']} erreurs")
    logger.info("=" * 60)

    # Fermeture du scraper Selenium si utilis√©
    global _dynamic_scraper
    if _dynamic_scraper is not None:
        _dynamic_scraper.close()
        _dynamic_scraper = None

    return resultats


def _mettre_a_jour_historique(categorie: str, nb_pages: int, nb_erreurs: int) -> None:
    """
    Met √† jour le fichier scrape_history.json apr√®s chaque cat√©gorie.

    Args:
        categorie: Nom de la cat√©gorie scrap√©e.
        nb_pages: Nombre de pages scrap√©es avec succ√®s.
        nb_erreurs: Nombre d'erreurs rencontr√©es.
    """
    historique = {}
    if HISTORY_FILE.exists():
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                historique = json.load(f)
        except (json.JSONDecodeError, OSError):
            historique = {}

    if "scrapes" not in historique:
        historique["scrapes"] = []

    historique["scrapes"].append({
        "categorie": categorie,
        "date": datetime.now(timezone.utc).isoformat(),
        "pages_scrapees": nb_pages,
        "erreurs": nb_erreurs,
    })

    # Garder les 100 derni√®res entr√©es
    historique["scrapes"] = historique["scrapes"][-100:]
    historique["derniere_mise_a_jour"] = datetime.now(timezone.utc).isoformat()

    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(historique, f, ensure_ascii=False, indent=2)


# ============================================================
# Point d'entr√©e pour ex√©cution directe
# ============================================================
if __name__ == "__main__":
    print("=" * 60)
    print("üîç L√âA ‚Äî Lancement du scraping des sources officielles")
    print("=" * 60)
    print()

    debut = time.time()
    resultats = scrape_all_sources()
    duree = time.time() - debut

    print()
    print(f"‚úÖ Scraping termin√© en {duree:.1f} secondes.")
    print(f"   {len(resultats)} pages collect√©es avec succ√®s.")
    print(f"   Fichiers Markdown enregistr√©s dans : {RAW_DIR}")
